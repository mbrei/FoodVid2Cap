{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate generated captions of the test set with unseen recipe types\n",
    "The captioning capabilities of the model are evaluated on an unseen test dataset which is compromised by \n",
    "7 recipe types that were never seen during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "import sys \n",
    "\n",
    "#specify root path for importing modules\n",
    "sys.path.append(\"C:/Users/User/foodcap/\")\n",
    "\n",
    "from src.utils import create_prediction_df, calculate_scores, WordCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the folder and model name \n",
    "model_folder = '../models/2019-08-02_17-12-01/'\n",
    "model_name = \"last_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data_all.csv\")\n",
    "\n",
    "json_pred_path = model_folder+ \"predictions_test_\"+model_name+\".json\"\n",
    "df = create_prediction_df(json_pred_path_zs,data, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation Scores\n",
    "The assessment of the model's captioning capability is\n",
    "recorded by several standard metrics that are commonly used in the field of natural \n",
    "language generation. These include the METEOR and ROUGE-L score. <br />\n",
    "Furthermore, the BLEU(1-4) and the CIDEr metric can be calculated. To calculate these scores download and work with the [pycocoeval package](https://github.com/tylin/coco-caption) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(calculate_scores,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METEOR Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10216201169789231"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.meteor.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE-L Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18153934275752437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rouge-l\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METEOR and ROUGE-L score reported for each recipe type separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meteor</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>burger</th>\n",
       "      <td>0.102162</td>\n",
       "      <td>0.181539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          meteor   rouge-l\n",
       "recipe                    \n",
       "burger  0.102162  0.181539"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"recipe\")[\"meteor\",\"rouge-l\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qualitative Results -  Look at the predicted words in each recipe type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCounter()\n",
    "words = wc.count_words_per_recipe(df_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burger\n",
      "        word  count  recipe        type  sum_count     ratio\n",
      "1        cut    191  burger  prediction        653  0.292496\n",
      "3       thin    177  burger  prediction        653  0.271057\n",
      "4   tomatoes    170  burger  prediction        653  0.260337\n",
      "7      aside     57  burger  prediction        653  0.087289\n",
      "9        set     37  burger  prediction        653  0.056662\n",
      "10    slices     13  burger  prediction        653  0.019908\n",
      "11     paper      8  burger  prediction        653  0.012251\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for recipe in words_zs.recipe.unique(): \n",
    "    \n",
    "    print(recipe)\n",
    "    print(words[(words.recipe == recipe)&(words.type == \"prediction\")].head(10))    \n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
